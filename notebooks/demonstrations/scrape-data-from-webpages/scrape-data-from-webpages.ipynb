{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch and Parse Webpage Data\n",
    "\n",
    "### *Requests and BeautifulSoup*\n",
    "\n",
    "---\n",
    "\n",
    "## Author\n",
    "\n",
    "[David J. Thomas](mailto:dave.a.base@gmail.com), [thePort.us](http://thePort.us)<br />\n",
    "Instructor of Ancient History and Digital Humanities<br />\n",
    "Department of History<br />\n",
    "[University of South Florida](https://github.com/usf-portal)\n",
    "\n",
    "---\n",
    "\n",
    "## Historical Source:\n",
    "Nearly 500 charters from Anglo-Saxon England, c. 600-900 and over 2,500 people who appear on them. Charters were elaborate documents containing grants of land, property, et.c. that always have a large number of very important witnesses which helped guaranteed its legitimacy. These charters, in aggregate, contain a wealth of information about reciprocity, relationships, and social display among medieval elites. In particular, each charter was signed by a list of witnesses. Between the texts of the charters and the names of the people on them, we can clean all kinds of useful information.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Source:\n",
    "Two databases. (1) The Anglo-Saxon Charters Database (ASC), the focus of our study. It contains the full text of hundreds of charters along with metadata. For this study, we will limit our purview to only these charters and only the individuals who appear in them. However, we need to round out our metadata on the charters and individuals. For that we will use (2) The Prosopography of Anglo-Saxon England Databases (PASE). Between these two we can get a significant amount of information on texts, people, and relationships.\n",
    "\n",
    "---\n",
    "\n",
    "## Packages Used\n",
    "\n",
    "* Requests\n",
    "    * [Main Documentation](https://python-requests.org)\n",
    "    * [PyPi Package](https://pypi.org/project/requests/)\n",
    "    * [GitHub Repo](https://github.com/psf/requests/)\n",
    "* BeautifulSoup\n",
    "    * [Main Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "    * [PyPi Package](https://pypi.org/project/beautifulsoup4/)\n",
    "\n",
    "*To understand more about Requests, I highly recommend the following*... [1](https://www.w3schools.com/python/module_requests.asp) | [2](https://scotch.io/tutorials/getting-started-with-python-requests-get-requests) | [3](https://www.pythonforbeginners.com/requests/using-requests-in-python)\n",
    "\n",
    "*To understand more about BeautifulSoup, I highly recommend the following*... [1](https://beautiful-soup-4.readthedocs.io/en/latest/) | [2](https://www.pythonforbeginners.com/beautifulsoup/beautifulsoup-4-python) | [3](https://www.datacamp.com/community/tutorials/scraping-reddit-python-scrapy)\n",
    "\n",
    "To FULLY understand everything, you probably need an intermediate Python skill. BUT, even if you have a beginners skill, you will be able to learn some useful tricks, work out larger strategies, and see the potential of Python for historical insight! If you want to learn how something is done, pay attention to the code documentation, or look up the official documentation on the package websites listed above, or at the tutorials I link inside the notebook. I hope this is useful to everyone from beginners to experts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1 - Basic Example\n",
    "\n",
    "Below shows the most basic usage in a script. First, a URL is sent to the `requests.get()` method, which sends back an HTML response object. That object contains many things, but part of it is the actual HTML that powers the webpage at the specified address. That data is stored in the `.text` property of the response object. This example simply fires off the request, gets the response object, stores the HTML, and then prints it to the screen.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to import the requests modules or it won't work\n",
    "import requests\n",
    "\n",
    "example_url = 'http://www.aschart.kcl.ac.uk/idc/idx_sawyerNo.html'\n",
    "\n",
    "# sent request to specified url and get back a response object\n",
    "example_response = requests.get(example_url)\n",
    "# single out the HTML and store the HTML at the .text property of the response object\n",
    "example_html = example_response.text\n",
    "\n",
    "# print result to screen to test\n",
    "print(example_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Automating with a Function\n",
    "\n",
    "In the last step we wrote a script that will get a single webpage. But you will want to get many pages. So, it is time to turn what we just did in the last step into a function, making it repeatable. Our function will get a URL and it will return the HTML from the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# always make sure to import needed packages\n",
    "import requests\n",
    "\n",
    "\n",
    "# time to define the function, remember to leave extra space above and below for good form.\n",
    "def get_page_html(webpage_url):\n",
    "    \"\"\"Gets a url, sends a web request, then returns the HTML from the response object.\"\"\"\n",
    "    webpage_response = requests.get(webpage_url)\n",
    "    webpage_html = webpage_response.text\n",
    "    return webpage_html\n",
    "\n",
    "\n",
    "# now let's test our function, let's set our example url again, call our function, then print the result\n",
    "example_url = 'http://www.aschart.kcl.ac.uk/idc/idx_sawyerNo.html'\n",
    "example_html = get_page_html(example_url)\n",
    "print(example_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Parsing HTML with BeautifulSoup\n",
    "\n",
    "Okay, great. So we've figured out how to quickly get the raw HTML for any website in a handful of lines of code. But, how do you extract that data? The raw HTML is a giant chunk of string data, and you just want to get a little bit of it. Usually just some text between particular HTML tags (the things that look like this <>), or some other small bit of data. Fortunately, `beautifulsoup4` helps you burrow down through this mass of text and search it inside of Python. You can search for just certain tags, or all tags matching certain criteria. You can then extract all kinds of information from them. So, first, let's do the most basic set, as we did in step 1 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU MUST HAVE RUN THE STEPS ABOVE FOR THIS TO WORK\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "example_url = 'http://www.aschart.kcl.ac.uk/idc/idx_sawyerNo.html'\n",
    "example_html = get_page_html(example_url)\n",
    "\n",
    "example_parsed = BeautifulSoup(example_html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def soupify_page(webpage_url):\n",
    "    webpage_response = requests.get(webpage_url)\n",
    "    webpage_html = webpage_response.text\n",
    "    webpage_parsed = BeautifulSoup(webpage_html, 'html.parser')\n",
    "    return webpage_parsed\n",
    "\n",
    "example_url = 'http://www.aschart.kcl.ac.uk/idc/idx_sawyerNo.html'\n",
    "example_parsed = soupify_page(example_url)\n",
    "\n",
    "print(example_parsed.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU MUST HAVE RUN THE STEPS ABOVE FOR THIS TO WORK\n",
    "\n",
    "example_url = 'http://www.aschart.kcl.ac.uk/idc/idx_sawyerNo.html'\n",
    "example_parsed = soupify_page(example_url)\n",
    "\n",
    "example_element = example_parsed.find('h2', id='d1605925e194')\n",
    "\n",
    "print(\"Entire Beautiful Soup element:\", example_element)\n",
    "print(\"Visible text inside element:\", example_element.text)\n",
    "print(\"Attribute value of element:\", example_element['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_url = 'http://www.aschart.kcl.ac.uk/idc/idx_sawyerNo.html'\n",
    "example_parsed = soupify_page(example_url)\n",
    "\n",
    "list_of_kingdoms = example_parsed.find_all('ul', class_='asc-expand')\n",
    "\n",
    "for kingdom in list_of_kingdoms[0:1]:\n",
    "    print(kingdom.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for kingdom in list_of_kingdoms:\n",
    "    kingdom_charters = kingdom.find_all('a')\n",
    "    for kingdom_charter in kingdom_charters[0:1]:\n",
    "        print(kingdom_charter.prettify())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_url = 'http://www.aschart.kcl.ac.uk/idc/idx_sawyerNo.html'\n",
    "example_parsed = soupify_page(example_url)\n",
    "\n",
    "list_of_kingdoms = example_parsed.find_all('ul', class_='asc-expand')\n",
    "\n",
    "charters = []\n",
    "for kingdom in list_of_kingdoms:\n",
    "    kingdom_charters = kingdom.find_all('a')\n",
    "    for kingdom_charter in kingdom_charters[0:5]:\n",
    "        print(kingdom_charter['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charter_urls = []\n",
    "\n",
    "for kingdom in list_of_kingdoms:\n",
    "    kingdom_charters = kingdom.find_all('a')\n",
    "    for kingdom_charter in kingdom_charters[0:5]:\n",
    "        print(kingdom_charter['href'][2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charter_urls = []\n",
    "\n",
    "for kingdom in list_of_kingdoms:\n",
    "    kingdom_charters = kingdom.find_all('a')\n",
    "    for kingdom_charter in kingdom_charters:\n",
    "        charter_url = 'http://www.aschart.kcl.ac.uk' + kingdom_charter['href'][2:]\n",
    "        charter_urls.append(charter_url)\n",
    "        \n",
    "for charter_url in charter_urls[0:10]:\n",
    "    print(charter_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going Object Oriented with Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class BasePage:\n",
    "    url = None\n",
    "\n",
    "    def __init__(self, url):\n",
    "        super().__init__()\n",
    "        self.url = url\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.url\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.url\n",
    "        \n",
    "    @property\n",
    "    def soup(self):\n",
    "        try:\n",
    "            webpage_response = requests.get(self.url)\n",
    "        except:\n",
    "            print('Problem fetching page at...', self.url)\n",
    "        webpage_html = webpage_response.text\n",
    "        webpage_parsed = BeautifulSoup(webpage_html, 'html.parser')\n",
    "        return webpage_parsed\n",
    "\n",
    "\n",
    "test_object = BasePage('http://www.aschart.kcl.ac.uk/idc/idx_sawyerNo.html')\n",
    "# This should just print the URL to the screen, the behavior we specified in .__str__() and .__repr__()\n",
    "print('The url for this test is', test_object)\n",
    "print('And it\\'s data is...')\n",
    "print(test_object.soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrowsePage(BasePage):\n",
    "    \n",
    "    def get_data(self):\n",
    "        page_html = self.soup\n",
    "        list_of_kingdoms = page_html.find_all('ul', class_='asc-expand')\n",
    "        for kingdom in list_of_kingdoms:\n",
    "            kingdom_charters = kingdom.find_all('a')\n",
    "            for kingdom_charter in kingdom_charters:\n",
    "                charter_url = 'http://www.aschart.kcl.ac.uk' + kingdom_charter['href'][2:]\n",
    "                charter_urls.append(charter_url)\n",
    "        return charter_urls\n",
    "\n",
    "\n",
    "test_object = BrowsePage('http://www.aschart.kcl.ac.uk/idc/idx_sawyerNo.html')\n",
    "# This should just print the URL to the screen, the behavior we specified in .__str__() and .__repr__()\n",
    "print('Scraping data from', test_object)\n",
    "test_urls = test_object.get_data()\n",
    "for test_url in test_urls[0:5]:\n",
    "    print(test_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharterPage(BasePage):\n",
    "    \n",
    "    def soup(self):\n",
    "        page_html = super().soup()\n",
    "        page_html.find('div', id='mainContent')\n",
    "\n",
    "\n",
    "test_object = BrowsePage('http://www.aschart.kcl.ac.uk/idc/idx_sawyerNo.html')\n",
    "# This should just print the URL to the screen, the behavior we specified in .__str__() and .__repr__()\n",
    "print('Scraping data from', test_object)\n",
    "test_urls = test_object.get_data()\n",
    "for test_url in test_urls[0:5]:\n",
    "    print(test_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
